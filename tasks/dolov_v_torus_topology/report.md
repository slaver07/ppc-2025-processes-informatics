# Решетка-тор

- **Student:** Долов Вячеслав Васильевич, group 3823Б1ФИ3
- **Technology:** SEQ | MPI
- **Variant:** 9

## 1. Introduction
Топология межпроцессорных соединений является определяющим фактором производительности распределенных систем. Одной из наиболее эффективных структур является **решетка-тор**. Она представляет собой двумерную сетку, в которой крайние узлы соединены с противоположными, что позволяет минимизировать диаметр сети и обеспечить несколько альтернативных путей для передачи данных.

**Цель работы:** Реализовать последовательную (SEQ) и параллельную (MPI) версии алгоритма маршрутизации в топологии решетка-тор с использованием возможностей MPI по работе с виртуальными топологиями и нахождением кратчайшего пути через границы.

---

## 2. Problem Statement
Задача заключается в моделировании и реализации процесса передачи сообщения (вектора целых чисел) от узла-отправителя к узлу-получателю в логической сети, организованной как тор.



**Входные данные:**
* Общее количество процессов: $P$
* Ранг отправителя: `sender_rank`
* Ранг получателя: `receiver_rank`
* Сообщение: `std::vector<int>` (для тестов производительности $N=10,000,000$)

**Выходные данные:**
* Полученное сообщение на узле-получателе.
* Полный маршрут следования сообщения (список рангов всех узлов пути).

---

## 3. Baseline Algorithm (Sequential)
Последовательный алгоритм выполняет виртуальную маршрутизацию в памяти:
1. **Геометрия:** Определяются размеры сетки $r \times c$ на основе общего числа процессов $P$ (размеры выбираются максимально близко к квадрату).
2. **Координаты:** Ранги преобразуются в декартовы координаты $(x, y)$ по формулам: $x = rank \pmod c$, $y = rank / c$.
3. **Маршрутизация:** - Вычисляется кратчайшее расстояние по оси $X$ с учетом циклического сдвига (сравнение пути напрямую и через границу).
   - Сообщение «перемещается» по горизонтали до нужного столбца.
   - Аналогично вычисляется кратчайший путь по оси $Y$ до нужной строки получателя.
4. **Сбор данных:** Каждый промежуточный шаг записывается в вектор маршрута `route`.

---

## 4. Parallelization Scheme
В параллельной версии сообщение физически передается между процессами MPI с использованием виртуальной топологии.
* **Топология:** На основе вычисленных координат строится распределенный граф со связями типа «тор» через `MPI_Dist_graph_create_adjacent`. Вместо глобального коммуникатора используется специализированный `torus_comm`.
* **Передача данных:** Узел, владеющий сообщением в данный момент, определяет следующего соседа на кратчайшем пути и отправляет ему данные через `MPI_Send`.
* **Прием и трансляция:** Промежуточные узлы принимают сообщение через `MPI_Recv`, добавляют свой ранг в маршрут и пересылают данные дальше следующему соседу.
* **Синхронизация:** После того как сообщение достигло получателя, финальный маршрут и полученные данные рассылаются всем остальным процессам через `MPI_Bcast` внутри созданного коммуникатора для обеспечения корректного завершения коллективной задачи во всех потоках.

---

## 5. Implementation Details
* **Структура:** Логика вынесена в специализированные классы `DolovVTorusTopologySEQ` и `DolovVTorusTopologyMPI`.
* **Ключевые функции:**
    - `DefineGridDimensions`: расчет оптимальных размеров сетки.
    - `FindShortestPathStep`: логика выбора направления (North, South, East, West) с учетом «склейки» границ тора.
    - `GetTargetNeighbor`: вычисление ранга соседа по направлению движения.
    - `MPI_Dist_graph_create_adjacent`: создание виртуальной топологии без использования `MPI_Cart_Create`.
* **Валидация:** Проверка корректности рангов отправителя/получателя и наличия данных в сообщении.
* **Коммуникации:** Использование специфических тегов `kDataTransfer` и `kRouteSync` для предотвращения смешивания данных сообщения и служебной информации о маршруте.

---

## 6. Experimental Setup
* **Hardware/OS:**
  - Ноутбук: Redmi Book Pro 16 2024
  - CPU: Intel(R) Core(TM) Ultra 5 125H (14 ядер, 18 потоков) @ 1.20 GHz
  - RAM: 32 GB
  - OS: Windows 11 Home
  - Среда выполнения: Dev Container (Docker, Ubuntu)
* **Toolchain:**
  - CMake 3.28.3
  - Компилятор: g++ 13.3.0
  - Библиотека: OpenMPI
  - Тип сборки: Release
* **Data:**
  - Размер сообщения: 10,000,000 целых чисел.
  - Количество процессов для замера: $P=4$.

---

## 7. Results and Discussion

### 7.1 Correctness
Корректность работы подтверждена прохождением **45 функциональных тестов**.
* Тесты включают сценарии: передача самому себе (`Self`), соседям по осям, через границы тора (`Wrap-around`) и на максимальные расстояния в сетке.
* Все тесты (и MPI, и SEQ) успешно завершены (`PASSED`), что доказывает идентичность и правильность логики маршрутизации.

### 7.2 Performance
Замеры времени для передачи 10,000,000 элементов на 4 процессах:

| Mode | Count ($P$) | Time, s | Speedup ($S_P$) | Efficiency ($E$) |
| :--- | :--- | :--- | :--- | :--- |
| **seq** | 1 | 0.0052311 | **1.00** | N/A |
| **mpi** | 4 | 0.0185421 | **0.08** | **2.0%** |

 В данной задаче наблюдается значительное замедление в MPI-версии. Это ожидаемо: вычислительная нагрузка (выбор одного из 4 направлений) ничтожна по сравнению с накладными расходами на копирование 40 МБ данных в сетевой стек и их передачу между процессами. MPI в данном случае используется для демонстрации логики распределенной сети, а не для ускорения вычислений.

---

## 8. Conclusions
Задача полностью решена. Реализована и протестирована модель маршрутизации в топологии 2D-тор с использованием виртуальных коммуникаторов.
* **Результат:** Программа успешно находит кратчайшие пути, эффективно используя «зацикленные» связи тора.
* **Вывод:** Физическая пересылка данных через MPI и использование распределенных графов подтвердили свою корректность. Для задач такого типа на одном узле последовательная реализация всегда будет быстрее из-за коммуникационных задержек.

---

## 9. References
1. Документация worldgonesour: Понятие Решетка-тор - https://worldgonesour.ru/stati/16732-reshetka-topologiya-kompyuternoy-seti.html.
2. Документация poznayka: Основные функции MPI - https://poznayka.org/s6430t1.html.
3. Лекции ННГУ по курсу "Параллельное программирование".

## Appendix
```cpp
// Логика выбора кратчайшего шага с учетом тороидальных связей:
int dx = tar_x - curr_x;
int dy = tar_y - curr_y;

// Кратчайший путь на торе (учитываем "склейку" границ)
if (std::abs(dx) > c / 2) dx = (dx > 0) ? dx - c : dx + c;
if (std::abs(dy) > r / 2) dy = (dy > 0) ? dy - r : dy + r;

// Сначала двигаемся по X, затем по Y
if (dx != 0) {
    return (dx > 0) ? MoveSide::kEast : MoveSide::kWest;
} else if (dy != 0) {
    return (dy > 0) ? MoveSide::kSouth : MoveSide::kNorth;
}
return MoveSide::kStay;