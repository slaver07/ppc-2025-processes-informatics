# Интегрирование – метод Монте-Карло
- Student: Долов Вячеслав Васильевич, group 3823Б1ФИ3
- Technology: SEQ | MPI
- Variant: 21

## 1. Introduction
Численное интегрирование многомерных функций является фундаментальной, но требовательной к ресурсам задачей. Метод Монте-Карло (ММК) — универсальный стохастический подход, точность которого напрямую зависит от количества случайных выборок ($N$). Необходимость обработки большого $N$ для достижения приемлемой точности делает ММК идеальной задачей для **распараллеливания** с помощью технологии **MPI**.

**Цель работы:** Реализовать последовательную (SEQ) и параллельную (MPI) версии ММК для интегрирования в областях **Гиперкуба** и **Гиперсферы**, и исследовать эффективность распараллеливания.

---

## 2. Problem Statement
Требуется вычислить значение определенного интеграла функции $f(\mathbf{x}) = \sum_{i=1}^D x_i^2$ на заданной многомерной области (домене) с заданным радиусом $R$ и центром.

**Входные данные:**
* Интегрируемая функция: $f(\mathbf{x})$
* Область интегрирования: `kHyperCube` или `kHyperSphere`
* Параметры области: Размерность $D$, центр и радиус $R$.
* Количество выборок: $N$ (для производительности $N=1,000,000$)

**Выходные данные:**
* Одно действительное число: приближенное значение интеграла.

---

## 3. Baseline Algorithm (Sequential)
Базовый алгоритм основан на оценке среднего значения функции $\langle f \rangle$ в области. Интеграл вычисляется по формуле: $I \approx V \cdot \langle f \rangle$.

1. **Генерация:** Сгенерировать $N$ случайных точек $\mathbf{x}_i$ в охватывающем гиперкубе.
2. **Обработка Домена:** - Если домен — **Гиперкуб**, точка всегда учитывается.
   - Если домен — **Гиперсфера**, проверяется условие нахождения точки внутри сферы радиуса $R$.
3. **Суммирование:** Вычислить сумму значений функции $f(\mathbf{x}_i)$ для всех точек, попавших в область.
4. **Результат:** I = V * Σ f(x_i) / N_total

---

## 4. Parallelization Scheme
Метод Монте-Карло демонстрирует идеальный параллелизм, поскольку каждая выборка независима.

**Схема:**
* **Декомпозиция данных:** Общее число выборок $N$ делится на $P$ процессов, каждый из которых выполняет свою порцию итераций.
* **Локальные Вычисления:** Каждый процесс независимо генерирует случайные точки и рассчитывает свою частичную сумму.
* **Коммуникация:** Для агрегации результатов используется коллективная операция **`MPI_Reduce`** с оператором `MPI_SUM`.
* **Роли:** Корневой процесс (`rank 0`) собирает все частичные суммы и выполняет финальный расчет интеграла.

---

## 5. Implementation Details
* **Структура:** Логика реализована в классах, наследующих `Task`, с разделением на `ops_seq.cpp` и `ops_mpi.cpp`.
* **Граничные случаи:** Реализована валидация входных данных (проверка корректности размерности и положительного количества выборок).
* **Контроль Качества:** Код отформатирован согласно правилам проекта с использованием `clang-format`.
* **Память:** Алгоритм потребляет константный объем памяти на каждом процессе, не требуя хранения координат всех точек.

---

## 6. Experimental Setup
* **Hardware/OS:**
  - Ноутбук: Redmi Book Pro 16 2024
  - CPU: Intel(R) Core(TM) Ultra 5 125H (14 ядер, 18 потоков) @ 1.20 GHz
  - RAM: 32 GB
  - OS: Windows 11 Home
  - Среда выполнения: Dev Container (Docker, Ubuntu)
* **Toolchain:**
  - CMake 3.28.3
  - Компилятор: g++ 13.3.0
  - Библиотека: OpenMPI
  - Тип сборки: Release
* **Data:**
  - Количество выборок: 1,000,000
  - Размерность: 3D
  - Функция: $f(\mathbf{x}) = \sum x_i^2$
  - Режим замера: Тесты производительности (P=1, P=4)

---

## 7. Results and Discussion

### 7.1 Correctness
Корректность подтверждена успешным прохождением **45 функциональных тестов**.
* Результаты MPI-версии совпадают с результатами последовательной версии в пределах статистической погрешности метода.
* Прохождение тестов подтверждает правильность работы логики для различных доменов и размерностей.

### 7.2 Performance
Замеры времени выполнения для $N=1,000,000$ выборок (3D куб):

| Mode | Count ($P$) | Time, s | Speedup ($S_P$) | Efficiency ($E$) |
| :--- | :--- | :--- | :--- | :--- |
| **seq** | 1 | $0.0287$ | **1.00** | N/A |
| **mpi** | 4 | $0.0072$ | $\mathbf{3.98}$ | $\mathbf{99.5\%}$ |

Полученное ускорение **3.98x** на 4 процессах является практически идеальным. Эффективность **99.5%** свидетельствует о том, что вычислительная нагрузка значительно преобладает над затратами на межпроцессное взаимодействие.

---

## 8. Conclusions
Задача полностью решена. Была успешно реализована и протестирована параллельная версия метода Монте-Карло.
* **Результат:** Достигнуто практически линейное ускорение на 4 процессах.
* **Вывод:** Распараллеливание методом декомпозиции итераций с использованием коллективных операций MPI является оптимальным способом решения задач стохастического интегрирования.

---

## 9. References
1. Документация poznayka: Основные функции MPI - https://poznayka.org/s6430t1.html.
2. Документация opennet: MPI для начинающих - https://www.opennet.ru/docs/RUS/MPI_intro/#Coll.
3. Документация habr: объяснение метода интегрирования Монте-Карло - https://habr.com/ru/articles/835870/.

## Appendix
```cpp
// Фрагмент кода, демонстрирующий сбор результатов:
double local_sum = 0.0;
// ... (Локальные вычисления на каждом процессе)
double global_sum = 0.0;

// Агрегация частичных сумм со всех процессов на корень (rank 0)
MPI_Reduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);

if (process_rank == 0) {
    // Финальный расчет: Объем * (Глобальная сумма / Общее число точек)
    double res = volume * (global_sum / total_samples);
}